---
title: AI-EDU-EASY的实验报告
date: 2020-12-2 21:30:12
tags: 线性回归实验报告
---

# 实验报告

##准备工作

**使用的库**:

```python
import numpy as np 
import math
from enum import Enum
import matplotlib.pyplot as plt
import os
from pathlib import Path
```

## 部分的代码

主函数：

```python
if __name__ == "__main__":
    url="Dataset/iris.csv"
    data=reader(url)
    data.read_csv_data()
    data.normalize_y(NetType.MultipleClassifier, base=1) 
    
    #fig= plt.figure(figsize=(6,6))
    data.normalize_x() 
    #data.shuffle()
    data.GenerateValidationSet()  
    n_input= data.num_feature
    n_ouput= data.num_category
    n_hidden=3  
    eta,batch_sz,max_epoch=0.1,20,5000
    eps=3
    hp=HyperParameters(n_input,n_ouput,n_hidden,eta,max_epoch,batch_sz,eps,
    NetType.MultipleClassifier, InitialMethod.Xavier)
    net= NeuralNet(hp,"I_don't_know")   
    net.load_result() 
    net.train(data,100,True)
```

读取csv文件：

```python
def read_csv_data(self):  #质检合格
        #file=csv.reader(self.path)
        data= np.loadtxt(self.path,delimiter=",",dtype="str",skiprows=1)
        #默认情况下，数据被认为是float类型，因此，我全转str
        self.xraw=data[:,:-1].astype("float")
        self.yraw=data[:,-1:]
        self.xtrain=self.xraw
        self.ytrain=self.yraw
        self.num_train=self.xtrain.shape[0]
        self.num_feature=self.xtrain.shape[1]
        self.num_category=len(np.unique(self.ytrain))#unique挺好用,下面这个就可以注释掉了
        self.label_dict={"Iris-setosa":0,"Iris-versicolor":1,"Iris-virginica":2}
        #好家伙，我自己分训练集
        self.test_head=int(self.num_train/4)
        self.test_end=int(self.num_train/2)
        self.xtest_raw=self.xraw[self.test_head:self.test_end,:]
        self.ytest_raw=self.yraw[self.test_head:self.test_end,:]
        self.xtest=self.xtest_raw
        self.ytest=self.ytest_raw
```

标准化：

```python
def __normalize_train_x(self,raw_data):
        x_new=np.zeros_like(raw_data)
        self.x_norm = np.zeros((2,self.num_feature))
        for i in range(self.num_feature):
            x=raw_data[:,i]  
            max_v=np.max(x)
            min_v=np.min(x)
            self.x_norm[0,i]=min_v
            self.x_norm[1,i]=max_v-min_v
            x_new[:,i]= (x - self.x_norm[0,i])/self.x_norm[1,i]
        return x_new
    def normalize_x(self):
        x_merge= np.vstack((self.xraw,self.xtest_raw))
        x_merge_norm= self.__normalize_train_x(x_merge)
        train_count= self.xraw.shape[0]
        self.xtrain= x_merge_norm[:train_count,:]
        self.xtest= x_merge_norm[train_count:,:]

    def normalize_y(self,type,base=0):
        if type == NetType.Fitting:
            y_merge = np.vstack((self.YTrainRaw, self.YTestRaw))
            y_merge_norm = self.__NormalizeY(y_merge)
            train_count = self.YTrainRaw.shape[0]
            self.YTrain = y_merge_norm[0:train_count,:]
            self.YTest = y_merge_norm[train_count:,:]                
        elif type == NetType.BinaryClassifier:
            self.YTrain = self.__ToZeroOne(self.YTrainRaw, base)
            self.YTest = self.__ToZeroOne(self.YTestRaw, base)
        if type == NetType.MultipleClassifier:  
            #我们这次重点关注 类型为MultipleClassifier的类型
            self.ytrain = self.__To_onehot(self.yraw, base)
            self.ytest = self.__To_onehot(self.ytest_raw, base)
    def normalize_predictdata(self,x_p): #尺度问题————待解决
        x_new_p=np.zeros(x_p.shape)
        for i in range(x_p.shape[1]):
            x_new_p[:,i]=(x_p[:,i]-self.x_norm[i,0])/self.x_norm[i,1]
        return x_new_p
        
    def __To_onehot(self,Y,base=0):
        count=Y.shape[0]
        y_new= np.zeros((count,self.num_category))
        for i in range(count):
            n=self.label_dict[Y[i,0]]
            y_new[i,base-n]=1
        return y_new
```

softmax:

```python
class softmax(object):
    def forward(self,z):
        shift_z= z-np.max(z,axis=1,keepdims=True)
        exp_z=np.exp(shift_z)
        a= exp_z/np.sum(exp_z,axis=1,keepdims=True)
        return a 
```

超参类：

```python
class HyperParameters(object):  #超参
    def __init__(self,n_input,n_output,n_hidden,eta=0.1,max_epoch=10000,batch_sz=5,
    eps=0.1,net_type=NetType.Fitting,init_method= InitialMethod.Xavier):
        self.num_input = n_input
        self.num_hidden = n_hidden
        self.num_output = n_output
        self.eta = eta
        self.max_epoch = max_epoch

        self.batch_sz = batch_sz
        self.net_type = net_type
        self.init_method = init_method
        self.eps = eps
```

神经网络类：

```python
class NeuralNet(object):
    def __init__(self,hp,nm):
        self.hp=hp
        self.model_name=nm
        self.subfolder=os.getcwd()+"\\"+self.__create_subfloder()
        #print(self.subfolder)  打印路径

        #self.w=np.zeros((self.hp.n_input,self.hp.n_output))
        #self.b=np.zeros((1,self.hp.n_output))

        self.wb1=WB(self.hp.num_input, self.hp.num_hidden, self.hp.init_method, self.hp.eta)
        self.wb1.init_w(self.subfolder, False)
        self.wb2=WB(self.hp.num_hidden, self.hp.num_output, self.hp.init_method, self.hp.eta)
        self.wb2.init_w(self.subfolder, False)

```

正向传播：

```python
def forward(self,batch_x):
        self.z1= np.dot(batch_x,self.wb1.w)+self.wb1.b #计算z1
        self.a1= Sigmoid().forward(self.z1) #计算a1
        self.z2= np.dot(self.a1,self.wb2.w)+self.wb2.b  #计算z2
        if self.hp.net_type ==NetType.BinaryClassifier:
            print("哈哈哈哈我没做二元分类")
        elif self.hp.net_type ==NetType.MultipleClassifier:
            self.a2= softmax().forward(self.z2) 
        else:
            self.a2= self.z2
        self.output= self.a2 #最终输出
```

反向传播：

```python
def backward(self,batch_x,batch_y,batch_a): #这个要注意
        m= batch_x.shape[0] #样本个数  
        dz2=self.a2-batch_y
        self.wb2.dw= np.dot(self.a1.T,dz2)#除以m防止梯度爆炸
        self.wb2.db= np.sum(dz2,axis=0, keepdims=True)/m
        #对于多样本计算，需要在横轴上做sum，得到平均值
        d1= np.dot(dz2, self.wb2.w.T)
        dz1,_ = Sigmoid().backward(None, self.a1, d1) #?????
        self.wb1.dw= np.dot(batch_x.T, dz1)/m
        self.wb1.db= np.sum(dz1)/m
```

训练：

```python
def train(self,dataread,checkpoint,need_test): #分类训练
        self.loss_trace= train_history() #历史记录
        self.loss_func= loss_func(self.hp.net_type) #loss对象
        if self.hp.batch_sz==-1:
            self.hp.batch_sz = dataread.num_train
        max_iteration= math.ceil(dataread.num_train/self.hp.batch_sz) #最大迭代数
        checkpoint_iteration= int(max_iteration*checkpoint)
        for epoch in range(self.hp.max_epoch):
            #dataread.shuffle()
            for iteration in range(max_iteration):
                batch_x,batch_y=dataread.get_batch_train(self.hp.batch_sz,iteration)
                batch_a= self.forward(batch_x)
                #正向传播结束
                self.backward(batch_x,batch_y,batch_a)
                #反向传播结束()
                self.updata()# 
                total_iteration = epoch * max_iteration + iteration #计算迭代数                
                if (total_iteration+1)% checkpoint_iteration == 0:
                    self.checkloss(dataread,batch_x,batch_y,epoch,total_iteration)
        self.save_result()
        print("end")
    def forward(self,batch_x):
        self.z1= np.dot(batch_x,self.wb1.w)+self.wb1.b #计算z1
        self.a1= Sigmoid().forward(self.z1) #计算a1
        self.z2= np.dot(self.a1,self.wb2.w)+self.wb2.b  #计算z2
        if self.hp.net_type ==NetType.BinaryClassifier:
            print("哈哈哈哈我没做二元分类")
        elif self.hp.net_type ==NetType.MultipleClassifier:
            self.a2= softmax().forward(self.z2) 
        else:
            self.a2= self.z2
        self.output= self.a2 #最终输出
```

检查loss:

```python
def checkloss(self,datareader,train_x,train_y,epoch,total_iteration):
        print(f"epoch={epoch}, total_iteration={total_iteration}")
        self.forward(train_x)
        #计算损失
        loss_train= self.loss_func.checkloss(self.output,train_y)
        accuracy_train= self.__cal_accuracy(self.output,train_y)
        print("loss_train=%.6f, accuracy_train=%f" %(loss_train, accuracy_train))
        #验证损失
        yld_x,yld_y= datareader.GetValidationSet()
        self.forward(yld_x)
        loss_vld= self.loss_func.checkloss(self.output,yld_y)
        accuracy_vld= self.__cal_accuracy(self.output,yld_y)
        print("loss_valid=%.6f, accuracy_valid=%f" %(loss_vld, accuracy_vld))        
        print(f"loss = {loss_train}")
```

## 实验结果

> 取了最后几次输出

```
......
loss = 0.037846634541078165
epoch=4599, total_iteration=32199
loss_train=0.037848, accuracy_train=1.000000
loss_valid=0.000179, accuracy_valid=1.000000
loss = 0.03784836217818403
epoch=4699, total_iteration=32899
loss_train=0.037850, accuracy_train=1.000000
loss_valid=0.000179, accuracy_valid=1.000000
loss = 0.03785007469182558
epoch=4799, total_iteration=33599
loss_train=0.037852, accuracy_train=1.000000
loss_valid=0.000179, accuracy_valid=1.000000
loss = 0.037851772317203346
epoch=4899, total_iteration=34299
loss_train=0.037853, accuracy_train=1.000000
loss_valid=0.000178, accuracy_valid=1.000000
loss = 0.03785345528533328
epoch=4999, total_iteration=34999
loss_train=0.037855, accuracy_train=1.000000
loss_valid=0.000178, accuracy_valid=1.000000
loss = 0.03785512382312461
```

